{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc836d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import pickle\n",
    "import mne\n",
    "import csv\n",
    "import re\n",
    "import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../preprocessing/utils/')\n",
    "import strf\n",
    "from utils import make_delayed, zs\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams as rc\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib import cm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "plt.style.use('seaborn')\n",
    "rc['pdf.fonttype'] = 42\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these path for running the notebook locally\n",
    "eeg_data_path = '/path/to/dataset/' # downloadable from OSF: https://doi.org/10.17605/OSF.IO/FNRD9\n",
    "git_path  = '/path/to/git/speaker_induced_suppression_EEG/'\n",
    "# Where the output of train_linear_model.ipynb is saved. Run that first if you haven't already.\n",
    "h5_path = '/path/to/h5/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b728eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "perception_color = '#117733'\n",
    "production_color = '#332288'\n",
    "all_color = 'gray'\n",
    "picks = ['F1','Fz','F2','FC1','FCz','FC2','C1','Cz','C2']\n",
    "tmin,tmax = -.3, .5\n",
    "delays = np.arange(np.floor(tmin*128),np.ceil(tmax*128),dtype=int)\n",
    "exclude = ['OP0001','OP0002','OP0004','OP0017','OP0020']\n",
    "subjs = np.sort([s[-6:] for s in glob(f'{git_path}eventfiles/*') if 'OP0' in s and s[-6:] not in exclude])\n",
    "models = ['model1','model1e','model2','model2e','model3','model3e','model4','model4e']\n",
    "single_model = 'model2'\n",
    "features = {model_number:strf.get_feats(model_number=model_number,extend_labels=True) for model_number in models}\n",
    "n_feats = {model_number:len(features[model_number]) for model_number in models}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd6bdcb",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a67c5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from hdf5, pandas\n",
    "wts, corrs, pvals, sig_wts, sig_corrs, alphas = dict(), dict(), dict(), dict(), dict(), dict()\n",
    "results_csv_fpath = f\"{git_path}stats/lem_results.csv\"\n",
    "df = pd.read_csv(results_csv_fpath)\n",
    "for m in models:\n",
    "    wts[m], corrs[m], pvals[m], sig_wts[m], sig_corrs[m], alphas[m] = dict(), dict(), dict(), dict(), dict(), dict()\n",
    "    b = tqdm(subjs)\n",
    "    for s in b:\n",
    "        blockid = f\"{s}_B1\"\n",
    "        b.set_description(f'Loading STRF for {s} {m}')\n",
    "        with h5py.File(f\"{h5_path}{s}_weights.hdf5\",'r') as f:\n",
    "            wts[m][s] = np.array(f.get(m))\n",
    "        ch_names = mne.io.read_raw_brainvision(f\"{eeg_data_path}{s}/{blockid}/{blockid}_cca.vhdr\",\n",
    "                                               preload=False,verbose=False).info['ch_names']\n",
    "        subj_corrs, subj_best_alphas, subj_pvals = np.zeros(len(ch_names)), np.zeros(len(ch_names)), np.zeros(len(ch_names))\n",
    "        for i, ch in enumerate(ch_names):\n",
    "            tgt_row = df[(df['subject']==s) & (df['model']==m) & (df['channel']==ch)]\n",
    "            subj_corrs[i] = df.loc[tgt_row.index, 'r_value']\n",
    "            subj_best_alphas[i] = df.loc[tgt_row.index, 'best_alpha']\n",
    "            subj_pvals[i] = df.loc[tgt_row.index, 'p_value']\n",
    "        corrs[m][s] = np.array(subj_corrs)\n",
    "        pvals[m][s] = np.array(subj_pvals)\n",
    "        alphas[m][s] = np.array(subj_best_alphas)\n",
    "    # Extract significant weights, corrs\n",
    "    for s in subjs:\n",
    "        nchans = wts[m][s].shape[2]\n",
    "        sig_wts[m][s] = np.zeros((len(delays),n_feats[m],nchans))\n",
    "        sig_corrs[m][s] = np.zeros((nchans))\n",
    "        for i in np.arange(nchans):\n",
    "            if pvals[m][s][i] < 0.01:\n",
    "                sig_wts[m][s][i] = wts[m][s][i]\n",
    "                sig_corrs[m][s][i] = corrs[m][s][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw from MNE vhdr\n",
    "raws, ch_names = dict(), dict()\n",
    "for s in subjs:\n",
    "    blockid = s + \"_B1\"\n",
    "    raws[s] = mne.io.read_raw_brainvision(f\"{eeg_data_path}{s}/{blockid}/{blockid}_cca.vhdr\",\n",
    "                                              preload=True,verbose=False)\n",
    "    ch_names[s] = raws[s].info['ch_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8e191",
   "metadata": {},
   "source": [
    "#### Calculate prediction of held-out EEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f79e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = dict()\n",
    "for s in subjs:\n",
    "    preds[s] = dict()\n",
    "    # Spkr pred\n",
    "    tmp_wts = wts[s][single_model][:,spkr_feats+all_feats,:]\n",
    "    n_delays, n_feats, n_chans = tmp_wts.shape\n",
    "    tmp_wts = tmp_wts.reshape((n_delays*n_feats,n_chans))\n",
    "    _,preds[s]['spkr_pred'] = strf.predict_response(tmp_wts,\n",
    "                                                   make_delayed(vStim[s][m][:,spkr_feats+all_feats], delays),\n",
    "                                                   zs(vResp[s][m]))\n",
    "    # Mic pred\n",
    "    tmp_wts = wts[s][single_model][:,mic_feats+all_feats,:]\n",
    "    n_delays, n_feats, n_chans = tmp_wts.shape\n",
    "    tmp_wts = tmp_wts.reshape((n_delays*n_feats,n_chans))\n",
    "    _,preds[s]['mic_pred'] = strf.predict_response(tmp_wts,\n",
    "                                                   make_delayed(vStim[s][m][:,mic_feats+all_feats], delays),\n",
    "                                                   zs(vResp[s][m]))\n",
    "    # All pred\n",
    "    tmp_wts = wts[s][single_model][:,all_feats,:]\n",
    "    n_delays, n_feats, n_chans = tmp_wts.shape\n",
    "    tmp_wts = tmp_wts.reshape((n_delays*n_feats,n_chans))\n",
    "    _,preds[s]['all_pred'] = strf.predict_response(tmp_wts,\n",
    "                                                   make_delayed(vStim[s][m][:,all_feats], delays),\n",
    "                                                   zs(vResp[s][m]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64926b",
   "metadata": {},
   "source": [
    "#### calculate weight correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_corrs = dict()\n",
    "n_feats = len(all_feats)\n",
    "for s in subjs:\n",
    "    n_delays, _, n_chans = wts[s][single_model].shape\n",
    "    spkr_wts = wts[s][single_model][:,spkr_feats,:]\n",
    "    mic_wts = wts[s][single_model][:,mic_feats,:]\n",
    "    wt_corrs[s] = np.zeros((n_feats,n_chans))\n",
    "    for ci in np.arange(n_chans):\n",
    "        for fi in np.arange(n_feats):\n",
    "            wt_corrs[s][fi,ci] = np.corrcoef(spkr_wts[:,fi,ci], mic_wts[:,fi,ci])[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1087d5dd",
   "metadata": {},
   "source": [
    "#### epoch the prediction data\n",
    "this may take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a59af",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_elecs = {\n",
    "    'OP0007':\"F7\",\n",
    "    \"OP0010\":\"FT7\",\n",
    "    \"OP0014\":\"F3\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b74ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sentence onsets\n",
    "all_onsets = dict()\n",
    "tmin, tmax = 0.0, 0.5\n",
    "for s in subjs:\n",
    "    ch_idx = ch_names[s].index(example_elecs[s])\n",
    "    blockid = s + \"_B1\"\n",
    "    resp = raws[s].get_data()\n",
    "    fs = raws[s].info['sfreq']\n",
    "    # Read event files\n",
    "    onsets, offsets, ids, = [], [], []\n",
    "    # Read event files (spkr)\n",
    "    spkr_sn_ev_fpath = f\"{git_path}eventfiles/{s}/{blockid}/{blockid}_spkr_sn_all.txt\"\n",
    "    with open(spkr_sn_ev_fpath,'r') as f:\n",
    "        c = csv.reader(f,delimiter='\\t')\n",
    "        for row in c:\n",
    "            onsets.append(int(float(row[0])*fs))\n",
    "            offsets.append(int(float(row[1])*fs))\n",
    "            ids.append(int(row[2]))\n",
    "    if s in ['OP0015','OP0016']:\n",
    "        # Load events from second block too\n",
    "        last_samp = mne.io.read_raw_brainvision(\n",
    "            f\"{eeg_data_path}{s}/{blockid}/{blockid}_downsampled.vhdr\",preload=False,verbose=False\n",
    "        ).last_samp\n",
    "        b2_blockid = f\"{s}_B2\"\n",
    "        b2_spkr_sn_ev_fpath = f\"{git_path}eventfiles/{s}/{b2_blockid}/{b2_blockid}_spkr_sn_all.txt\"\n",
    "        with open(b2_spkr_sn_ev_fpath,'r') as f:\n",
    "            c = csv.reader(f,delimiter='\\t')\n",
    "            for row in c:\n",
    "                onsets.append(int(float(row[0])*fs)+last_samp)\n",
    "                offsets.append(int(float(row[1])*fs)+last_samp)\n",
    "                ids.append(int(row[2]))\n",
    "    # Read event files (mic)\n",
    "    mic_sn_ev_fpath = f\"{git_path}eventfiles/{s}/{blockid}/{blockid}_mic_sn_all.txt\"\n",
    "    with open(mic_sn_ev_fpath,'r') as f:\n",
    "        c = csv.reader(f,delimiter='\\t')\n",
    "        for row in c:\n",
    "            onsets.append(int(float(row[0])*fs))\n",
    "            offsets.append(int(float(row[1])*fs))\n",
    "            ids.append(int(row[2]))\n",
    "    if s in ['OP0015','OP0016']:\n",
    "        # Load events from second block too\n",
    "        b2_mic_sn_ev_fpath = f\"{git_path}eventfiles/{s}/{b2_blockid}/{b2_blockid}_mic_sn_all.txt\"\n",
    "        with open(b2_mic_sn_ev_fpath,'r') as f:\n",
    "            c = csv.reader(f,delimiter='\\t')\n",
    "            for row in c:\n",
    "                onsets.append(int(float(row[0])*fs)+last_samp)\n",
    "                offsets.append(int(float(row[1])*fs)+last_samp)\n",
    "                ids.append(int(row[2]))\n",
    "    # Split events sentence-by-sentence\n",
    "    sn_events = dict()\n",
    "    for this_sentence in range(len(np.unique(ids))):\n",
    "        sn_ranges = []\n",
    "        for i, sn_id in enumerate(ids):\n",
    "            if sn_id == this_sentence:\n",
    "                onset_samp = onsets[i]\n",
    "                offset_samp = offsets[i]\n",
    "                sn_ranges.append([onset_samp,offset_samp])\n",
    "        sn_events[this_sentence] = sn_ranges        \n",
    "    # Split stim/resp sentence-by-sentence\n",
    "    resp_dict, sn_onsets_dict, sn_offsets_dict = dict(), dict(), dict()\n",
    "    for this_sentence in range(len(np.unique(ids))):\n",
    "        sn_resps, onset_matches, offset_matches = [], [], []\n",
    "        for i, ev in enumerate(sn_events[this_sentence]):\n",
    "            onset = ev[0]\n",
    "            offset = ev[1]\n",
    "            for samp_idx in np.arange(resp.shape[1]):\n",
    "                if samp_idx >= onset and samp_idx <= offset:\n",
    "                    sn_resps.append(resp[:,samp_idx])\n",
    "                    if samp_idx == onset:\n",
    "                        onset_matches.append(1)\n",
    "                    else:\n",
    "                        onset_matches.append(0)\n",
    "        resp_dict[this_sentence] = np.array(sn_resps)\n",
    "        sn_onsets_dict[this_sentence] = np.array(onset_matches)\n",
    "        sn_offsets_dict[this_sentence] = np.array(offset_matches)\n",
    "    # Split stim/resp into training/validation sets along sentence boundaries\n",
    "    nsentences = 50\n",
    "    tv_split = 40 # 40 sentences IDs to train, remaining 10 to validate\n",
    "    np.random.seed(6655321)\n",
    "    val_sn_ids = np.random.permutation(nsentences)[tv_split:]\n",
    "    vResps_by_sn, vResp_onsets, vResp_offsets = dict(), dict(), dict()\n",
    "    for this_sentence in val_sn_ids:\n",
    "        vResps_by_sn[this_sentence] = resp_dict[this_sentence]\n",
    "        vResp_onsets[this_sentence] = sn_onsets_dict[this_sentence]\n",
    "        vResp_offsets[this_sentence] = sn_offsets_dict[this_sentence]\n",
    "    # Reshape vResp/pred so they're sentence-by-sentence and \"epoch\"\n",
    "    all_onsets[s] = np.where(np.hstack((list(vResp_onsets.values())))==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff65d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Epoch\" to sentence onsets\n",
    "epochs = dict()\n",
    "for s in subjs:\n",
    "    epochs[s] = dict()\n",
    "    ch_idx = ch_names[s].index(example_elecs[s])\n",
    "    for p in preds[s].keys():\n",
    "        sentences = []\n",
    "        for i, samp in enumerate(all_onsets[s]):\n",
    "            onset = samp - np.abs(tmin*fs).astype(int)\n",
    "            offset = samp + int(tmax*fs)\n",
    "            sentences.append(preds[s][p][onset:offset,ch_idx])\n",
    "        epochs[s][p] = zs(np.array(sentences))\n",
    "    # Epoch the actual resp too\n",
    "    sentences = []\n",
    "    for i, samp in enumerate(all_onsets[s]):\n",
    "        onset = samp - np.abs(tmin*fs).astype(int)\n",
    "        offset = samp + int(tmax*fs)\n",
    "        sentences.append(vResp[s][single_model][onset:offset,ch_idx])\n",
    "        epochs[s]['vResp'] = zs(np.array(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29334103",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f2748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sem(epochs):\n",
    "    '''\n",
    "    calculates standard error margin across epochs\n",
    "    epochs should have shape (epochs,samples)\n",
    "    '''\n",
    "    sem_below = epochs.mean(0) - (epochs.std(0)/np.sqrt(epochs.shape[0]))\n",
    "    sem_above = epochs.mean(0) + (epochs.std(0)/np.sqrt(epochs.shape[0]))\n",
    "    return sem_below, sem_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(tmin,tmax,epochs[s]['vResp'].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec881c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Grand average\" prediction plots\n",
    "for s in subjs:\n",
    "    plt.figure(figsize=(7,3))\n",
    "    ch_idx = ch_names[s].index(example_elecs[s])\n",
    "    if s == 'OP0010':\n",
    "        ymin = epochs[s]['all_pred'].mean(0).min() * 1.5\n",
    "        ymax = -ymin\n",
    "#         ymax = epochs[s]['all_pred'].mean(0).max() * 1.5\n",
    "    elif s == 'OP0007':\n",
    "        ymin = epochs[s]['all_pred'].mean(0).min() * 1.08\n",
    "        ymax = -ymin\n",
    "    else:\n",
    "        ymin = epochs[s]['all_pred'].mean(0).min() * 1.15\n",
    "        ymax = -ymin\n",
    "#         ymax = epochs[s]['all_pred'].mean(0).max() * 1.15\n",
    "    # Spkr vs all\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(x,epochs[s]['spkr_pred'].mean(0),color=spkr_color)\n",
    "    plt.fill_between(x,sem(epochs[s]['spkr_pred'])[0],sem(epochs[s]['spkr_pred'])[1],color=spkr_color,alpha=0.3)\n",
    "    plt.plot(x,epochs[s]['all_pred'].mean(0),color=all_color)\n",
    "    plt.fill_between(x,sem(epochs[s]['all_pred'])[0],sem(epochs[s]['all_pred'])[1],color=all_color,alpha=0.3)\n",
    "    plt.gca().set_xlim(x[0],x[-1])\n",
    "    plt.gca().set_ylim([ymin,ymax])\n",
    "    plt.xlabel(\"Time (s)\",fontsize=10)\n",
    "    plt.ylabel(\"Z-scored EEG ± SEM\", fontsize=10)\n",
    "    plt.gca().set_yticks([ymin,ymin/2,0,ymax/2,ymax])\n",
    "    plt.gca().set_yticklabels([\"%.2f\"%ymin,'',0,'',\"%.2f\"%ymax])\n",
    "    plt.gca().set_xticks([tmin,tmax*0.25,tmax*0.5,tmax*0.75,tmax])\n",
    "    plt.gca().set_xticklabels([tmin,'','','',tmax])\n",
    "    \n",
    "    # mic vs all\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(x,epochs[s]['mic_pred'].mean(0),color=mic_color)\n",
    "    plt.fill_between(x,sem(epochs[s]['mic_pred'])[0],sem(epochs[s]['mic_pred'])[1],color=mic_color,alpha=0.3)\n",
    "    plt.plot(x,epochs[s]['all_pred'].mean(0),color=all_color)\n",
    "    plt.fill_between(x,sem(epochs[s]['all_pred'])[0],sem(epochs[s]['all_pred'])[1],color=all_color,alpha=0.3)\n",
    "    plt.gca().set_ylim([ymin,ymax])\n",
    "    plt.gca().set_yticklabels([])\n",
    "    plt.gca().set_xticklabels([])\n",
    "    plt.gca().set_xlim(x[0],x[-1])\n",
    "    plt.gca().set_yticks([ymin,ymin/2,0,ymax/2,ymax])\n",
    "    plt.gca().set_xticks([tmin,tmax*0.25,tmax*0.5,tmax*0.75,tmax])\n",
    "    plt.gca().set_xticklabels([tmin,'','','',tmax])\n",
    "\n",
    "    \n",
    "    # spkr vs mic\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(x,epochs[s]['spkr_pred'].mean(0),color=spkr_color)\n",
    "    plt.fill_between(x,sem(epochs[s]['spkr_pred'])[0],sem(epochs[s]['spkr_pred'])[1],color=spkr_color,alpha=0.3)\n",
    "    plt.plot(x,epochs[s]['mic_pred'].mean(0),color=mic_color)\n",
    "    plt.fill_between(x,sem(epochs[s]['mic_pred'])[0],sem(epochs[s]['mic_pred'])[1],color=mic_color,alpha=0.3)\n",
    "    plt.gca().set_xlim(x[0],x[-1])\n",
    "    plt.gca().set_yticklabels([])\n",
    "    plt.gca().set_xticklabels([])\n",
    "    plt.gca().set_ylim([ymin,ymax])\n",
    "    plt.gca().set_yticks([ymin,ymin/2,0,ymax/2,ymax])\n",
    "    plt.gca().set_xticks([tmin,tmax*0.25,tmax*0.5,tmax*0.75,tmax])\n",
    "    plt.gca().set_xticklabels([tmin,'','','',tmax])\n",
    "\n",
    "    \n",
    "    plt.suptitle(f\"{s} {example_elecs[s]}\",fontsize=12)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714fad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legend\n",
    "plt.figure(figsize=(7,1))\n",
    "plt.axis(False)\n",
    "plt.bar(0,0,color=spkr_color,label=\"Pred. EEG (perception-specific weights)\")\n",
    "plt.bar(0,0,color=mic_color,label=\"Pred. EEG (production-specific weights)\")\n",
    "plt.bar(0,0,color=all_color,label=\"Pred. EEG (combined weights)\")\n",
    "plt.legend(fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da1248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight correlations\n",
    "corr_vmin = -1\n",
    "corr_vmax = 0\n",
    "corr_cmap = LinearSegmentedColormap.from_list('my_gradient', (\n",
    "    # Edit this gradient at https://eltos.github.io/gradient/#41047F-EAEAF2\n",
    "    (0.000, (0.255, 0.016, 0.498)),\n",
    "    (1.000, (0.918, 0.918, 0.949))))\n",
    "for s in subjs:\n",
    "    ch_idx = ch_names[s].index(example_elecs[s])\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    gs = GridSpec(8, 17, figure=fig)\n",
    "    axes = []\n",
    "    # Spkr feats\n",
    "    axes.append(fig.add_subplot(gs[:,:8]))\n",
    "    ch_wts = wts[s][single_model][:,spkr_feats,ch_idx]\n",
    "    vmax = ch_wts.max()\n",
    "    plt.imshow(ch_wts.T,\n",
    "           aspect='auto',interpolation='nearest',vmin=-vmax,vmax=vmax,cmap=cm.RdBu_r)\n",
    "    plt.gca().set_yticks(np.arange(n_feats))\n",
    "    plt.gca().set_yticklabels(features[:14])\n",
    "    plt.gca().set_xticks([0,np.where(delays==0)[0][0],n_delays])\n",
    "    plt.gca().set_xticklabels([delay_min,0.0,delay_max])\n",
    "    plt.axvline(np.where(delays==0)[0][0],color='k')\n",
    "    plt.grid(False)\n",
    "    plt.title(\"Perception-specific phnfeat\")\n",
    "    # Correlation\n",
    "    axes.append(fig.add_subplot(gs[:,8]))\n",
    "    plt.imshow(np.expand_dims(wt_corrs[s][:,ch_idx],axis=0).T,\n",
    "               aspect='auto',interpolation='nearest',vmin=corr_vmin,vmax=corr_vmax,cmap=corr_cmap)\n",
    "    plt.gca().set_xticks([])\n",
    "    plt.gca().set_yticks([])\n",
    "    plt.title(\"corr\")\n",
    "    plt.grid(False)\n",
    "    # Mic feats\n",
    "    axes.append(fig.add_subplot(gs[:,9:]))\n",
    "    ch_wts = wts[s][single_model][:,mic_feats,ch_idx]\n",
    "    vmax = ch_wts.max()\n",
    "    plt.imshow(ch_wts.T,\n",
    "           aspect='auto',interpolation='nearest',vmin=-vmax,vmax=vmax,cmap=cm.RdBu_r)\n",
    "    plt.axvline(np.where(delays==0)[0][0],color='k')\n",
    "    plt.gca().set_yticks([])\n",
    "    plt.gca().set_xticks([])\n",
    "    plt.grid(False)\n",
    "    plt.title(\"Production-specific phnfeat\")\n",
    "    plt.suptitle(f\"{s} {ch_names[s][ch_idx]}\", fontsize=12)\n",
    "    gs.tight_layout(figure=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# legend\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.zeros((1,1)),aspect='auto',interpolation='nearest',cmap=corr_cmap,vmin=corr_vmin,vmax=corr_vmax)\n",
    "plt.gca().set_visible(False)\n",
    "plt.colorbar(orientation='horizontal');\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(np.zeros((1,1)),aspect='auto',interpolation='nearest',cmap=cm.RdBu_r)\n",
    "plt.gca().set_visible(False)\n",
    "plt.colorbar(orientation='horizontal');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce6f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plots\n",
    "plt.figure(figsize=(2,9))\n",
    "sns.violinplot(data=pd.DataFrame.from_dict(all_wt_corrs,orient='index').T,\n",
    "               orient='h', palette=['#b2b2b2'] * len(subjs), scale='width')\n",
    "plt.axvline(0,color='k',ls='--',lw=0.5)\n",
    "plt.gca().set_yticklabels(subjs,fontsize=8)\n",
    "plt.ylabel(\"Subject\",fontsize=12)\n",
    "plt.gca().set_xlim([-1.15,1.15])\n",
    "plt.gca().set_xticks([-1,-.5,0,.5,1])\n",
    "plt.gca().set_xticklabels([-1.0,-.5,0.0,0.5,1.0],fontsize=8)\n",
    "plt.xlabel(\"r\",fontsize=12)\n",
    "plt.title(\"Correlation of production-specific\\nand perception-specific weights\", fontsize=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
