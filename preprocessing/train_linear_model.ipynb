{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "539e8b7e",
   "metadata": {},
   "source": [
    "## Train linear model\n",
    "This notebook fits linear encoding models to phoneme level data using different stimulus matrices, depending on the analysis in question. Here is a useful shorthand for the different model constructions:\n",
    "* Model 1: 14 phonological features + 4 task features + normalized EMG\n",
    "* Model 2: 42 phonological features (14 perception only, 14 production only, 14 combined) + 4 task features + normalized EMG\n",
    "* Model 3: 14 phonological features + 2 task features (predictability contrast omitted) + normalized EMG\n",
    "* Model 4: 14 phonological features + 2 task features (perception/production contrast omitted) + normalized EMG\n",
    "\n",
    "Any model ending in `e` is the same as the above, but _without_ an EMG regressor.\n",
    "\n",
    "**Warning: this notebook is very computationally intensive.** You may wish to run it on your lab server instead of a local machine. Another alternative is to split the subjects up into smaller subsets, then run those subsets instead of holding all the subjects' models in memory at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f6266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import mne\n",
    "import re\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from matplotlib import cm,rcParams\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "sys.path.append('./utils/')\n",
    "import textgrid\n",
    "import strf\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4583f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local paths\n",
    "eeg_data_path = '/path/to/dataset/' # downloadable from OSF: https://doi.org/10.17605/OSF.IO/FNRD9\n",
    "git_path  = '/path/to/git/speaker_induced_suppression_EEG/'\n",
    "h5_path = '/path/to/h5/' # Where the model inputs and results will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f67aaa1",
   "metadata": {},
   "source": [
    "### Initialize model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dbeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = ['OP0001','OP0002','OP0020'] # OP1/2 don't have Aux EMG; OP20 had recording error/is excluded overall\n",
    "subjs = np.sort([s[-6:] for s in glob(f'{git_path}eventfiles/*') if 'OP0' in s and s[-6:] not in exclude])\n",
    "condition, level = 'all', 'phoneme'\n",
    "model_number = 'model1'\n",
    "tmin,tmax = -0.3,0.5\n",
    "delays = np.arange(np.floor(tmin*128),np.ceil(tmax*128),dtype=int)\n",
    "ndelays = len(delays)\n",
    "nboots = 10\n",
    "alphas=np.hstack((0, np.logspace(-4,4,20)))\n",
    "features_dict = {\n",
    "                'dorsal': ['y','w','k','kcl', 'g','gcl','eng','ng'],\n",
    "                'coronal': ['ch','jh','sh','zh','s','z','t','tcl','d','dcl','n','th','dh','l','r'],\n",
    "                'labial': ['f','v','p','pcl','b','bcl','m','em','w'],\n",
    "                'high': ['uh','ux','uw','iy','ih','ix','ey','eh','oy'],\n",
    "                'front': ['iy','ih','ix','ey','eh','ae','ay'],\n",
    "                'low': ['aa','ao','ah','ax','ae','aw','ay','axr','ow','oy'],\n",
    "                'back': ['aa','ao','ow','ah','ax','ax-h','uh','ux','uw','axr','aw'],\n",
    "                'plosive': ['p','pcl','t','tcl','k','kcl','b','bcl','d','dcl','g','gcl','q'],\n",
    "                'fricative': ['f','v','th','dh','s','sh','z','zh','hh','hv','ch','jh'],\n",
    "                'syllabic': ['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ax-h', 'axr', 'ay','eh','ey','ih', 'ix',\n",
    "                             'iy','ow', 'oy','uh', 'uw', 'ux'],\n",
    "                'nasal': ['m','em','n','en','ng','eng','nx'],\n",
    "                'voiced': ['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ax-h', 'axr', 'ay','eh','ey','ih', 'ix',\n",
    "                             'iy','ow', 'oy','uh', 'uw', 'ux','w','y','el','l','r','dh','z','v','b','bcl','d',\n",
    "                             'dcl','g','gcl','m','em','n','en','eng','ng','nx','q','jh','zh'],\n",
    "                'obstruent': ['b', 'bcl', 'ch', 'd', 'dcl', 'dh', 'dx','f', 'g', 'gcl', 'hh', 'hv','jh', 'k',\n",
    "                              'kcl', 'p', 'pcl', 'q', 's', 'sh','t', 'tcl', 'th','v','z', 'zh','q'],\n",
    "                'sonorant': ['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ax-h', 'axr', 'ay','eh','ey','ih', 'ix',\n",
    "                             'iy','ow', 'oy','uh', 'uw', 'ux','w','y','el','l','r','m',\n",
    "                             'n', 'ng', 'eng', 'nx','en','em'],\n",
    "        }\n",
    "features = [f for f in features_dict.keys()]\n",
    "print(features)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f7fb0",
   "metadata": {},
   "source": [
    "### Load raw data\n",
    "(and aux EMG, assuming we want it in our specified model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31fd759",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws, emgs = dict(), dict()\n",
    "for s in tqdm(subjs):\n",
    "    blockid = f'{s}_B1'\n",
    "    fname = f'{eeg_data_path}{s}/{blockid}/{blockid}_cca.vhdr'\n",
    "    raws[s] = mne.io.read_raw_brainvision(fname,preload=True,verbose=False)\n",
    "    raws[s].filter(l_freq=1, h_freq=30)\n",
    "    if model_number[-1] != 'e': # if we want EMG, get it\n",
    "        ica_fname = f'{eeg_data_path}{s}/{blockid}/{blockid}_ica.fif'\n",
    "        ica_raw = mne.io.read_raw_fif(ica_fname, preload=True)\n",
    "        if 'hEOG' in ica_raw.info['ch_names']:\n",
    "            ica_raw.notch_filter(60,picks=['hEOG'],verbose=False)\n",
    "            ica_raw.filter(l_freq=1,h_freq=None,picks=['hEOG'],verbose=False)\n",
    "            emg = ica_raw.get_data(picks=['hEOG'])\n",
    "            emgs[s] = emg/np.abs(emg).max() # Scale EMG\n",
    "        else:\n",
    "            raise Exception('EMG not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad71acd9",
   "metadata": {},
   "source": [
    "### Load events from eventfiles\n",
    "Although only phoneme-level linear models are fit in the paper, this notebook can load sentence- and word-level stimuli as well, as it's possible to fit those.\n",
    "\n",
    "Eventfiles are named according to channel, level, and condition:\n",
    "* **Channel**: the mode of speech used during a specific trial.\n",
    "    * `mic` eventfiles contain events from the production component of the task.\n",
    "    * `spkr` eventfiles contain events from the perception component of the task.\n",
    "* **Level**: the level of linguistic representation encoded in the eventfile\n",
    "    * `ph`: individual phoneme events\n",
    "    * `wr`: word-level events\n",
    "    * `sn`: sentence-level events\n",
    "* **Condition**: the type of playback utilized in the perception trials\n",
    "    * `el` eventfiles contain the consistent playback trials or their preceding production trials (depends on specified channel).\n",
    "    * `sh` eventfiles contain the inconsistent playback trials or their preceding production trials (depends on specified channel).\n",
    "    * `all` eventfiles contain both the consistent and inconsistent playback trials, or their preceding production trials (depending on specified channel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e45b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition, level = 'all', 'ph'\n",
    "spkr_events, mic_events = dict(), dict()\n",
    "pbar = tqdm(subjs)\n",
    "for s in pbar:\n",
    "    pbar.set_description(f\"Loading events for {s}\")\n",
    "    blockid = f\"{s}_B1\"\n",
    "    fs = raws[s].info['sfreq']\n",
    "    # Spkr events\n",
    "    event_fpath = f\"{git_path}eventfiles/{s}/{blockid}/{blockid}_spkr_{level}_{condition}.txt\"\n",
    "    event_file = []\n",
    "    with open(event_fpath,'r') as f:\n",
    "        c = csv.reader(f,delimiter='\\t')\n",
    "        for row in c:\n",
    "            if level == 'ph':\n",
    "                event_file.append(row[:3]+row[4:])\n",
    "            else:\n",
    "                event_file.append(row[:3])\n",
    "    event_file = np.array(event_file,dtype=float)\n",
    "    event_file[:,:2] = np.round(event_file[:,:2]*fs)\n",
    "    if s in ['OP0015','OP0016']:\n",
    "        # These two subjects have multiple blocks and those needed to be added to the events list\n",
    "        b2_blockid = f\"{s}_B2\"\n",
    "        b2_event_fpath = f\"{git_path}eventfiles/{s}/{b2_blockid}/{b2_blockid}_spkr_{level}_{condition}.txt\"\n",
    "        b2_events = []\n",
    "        with open(b2_event_fpath, 'r') as f:\n",
    "            c = csv.reader(f,delimiter='\\t')\n",
    "            for row in c:\n",
    "                if level == 'ph':\n",
    "                    b2_events.append(row[:3]+row[4:])\n",
    "                else:\n",
    "                    b2_events.append(row[:3])\n",
    "        b2_events = np.array(b2_events,dtype=float)\n",
    "        b2_events[:,:2] = np.round(b2_events[:,:2]*fs)\n",
    "        last_samp = mne.io.read_raw_brainvision(\n",
    "            f\"{eeg_data_path}{s}/{blockid}/{blockid}_downsampled.vhdr\",preload=False,verbose=False\n",
    "        ).last_samp\n",
    "        b2_events[:,0] = b2_events[:,0]+last_samp\n",
    "        b2_events[:,1] = b2_events[:,1]+last_samp\n",
    "        event_file = np.vstack((event_file,b2_events))\n",
    "    spkr_events[s] = event_file.astype(int)\n",
    "    # Mic events (need non-task times removed)\n",
    "    event_fpath = f\"{git_path}eventfiles/{s}/{blockid}/{blockid}_mic_{level}_{condition}.txt\"\n",
    "    event_file = []\n",
    "    with open(event_fpath,'r') as f:\n",
    "        c = csv.reader(f,delimiter='\\t')\n",
    "        for row in c:\n",
    "            if level == 'ph':\n",
    "                event_file.append(row[:3]+row[4:])\n",
    "            else:\n",
    "                event_file.append(row[:3])\n",
    "    event_file = np.array(event_file,dtype=float)\n",
    "    event_file[:,:2] = np.round(event_file[:,:2]*fs)\n",
    "    if s in ['OP0015','OP0016']:\n",
    "        # These two subjects have multiple blocks and those needed to be added to the events list\n",
    "        b2_event_fpath = f\"{git_path}eventfiles/{s}/{b2_blockid}/{b2_blockid}_mic_{level}_{condition}.txt\"\n",
    "        b2_events = []\n",
    "        with open(b2_event_fpath, 'r') as f:\n",
    "            c = csv.reader(f,delimiter='\\t')\n",
    "            for row in c:\n",
    "                if level == 'ph':\n",
    "                    b2_events.append(row[:3]+row[4:])\n",
    "                else:\n",
    "                    b2_events.append(row[:3])\n",
    "        b2_events = np.array(b2_events,dtype=float)\n",
    "        b2_events[:,:2] = np.round(b2_events[:,:2]*fs)\n",
    "        b2_events[:,0] = b2_events[:,0]+last_samp\n",
    "        b2_events[:,1] = b2_events[:,1]+last_samp\n",
    "        event_file = np.vstack((event_file,b2_events))\n",
    "    mic_tg_path = f\"{git_path}textgrids/{s}/{blockid}/{blockid}_mic.textgrid\"\n",
    "    with open(mic_tg_path) as r:\n",
    "        tg = textgrid.TextGrid(r.read())\n",
    "    task_times = np.array([t*fs for t in gk.get_task_times(tg)],dtype=int)\n",
    "    if s in ['OP0015','OP0016']:\n",
    "        # Load textgrid from second block as well\n",
    "        b2_mic_tg_path = f\"{git_path}textgrids/{s}/{b2_blockid}/{b2_blockid}_mic.textgrid\"\n",
    "        with open(b2_mic_tg_path) as r:\n",
    "            b2_tg = textgrid.TextGrid(r.read())\n",
    "        b2_task_times = np.array([(t*fs)+last_samp for t in gk.get_task_times(tg)],dtype=int)\n",
    "        task_times = np.vstack((task_times,b2_task_times))\n",
    "    task_range = [np.arange(t[0],t[1],step=1) for t in task_times]\n",
    "    trial_events = []\n",
    "    for d in task_range:\n",
    "        for ev in event_file:\n",
    "            onset = ev[0]\n",
    "            offset = ev[1]\n",
    "            if onset in d and offset in d:\n",
    "                trial_events.append(ev)\n",
    "    mic_events[s] = np.array(trial_events).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eae23d",
   "metadata": {},
   "source": [
    "### Create a stimulus matrix and a response matrix to pass into linear encoding model\n",
    "* Stimulus matrix has a shape of `n_eeg_samples` x `n_features`\n",
    "* Response matrix has a shape of `n_eeg_channels` x `n_eeg_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3456b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stims,resps = dict(),dict()\n",
    "phonemes = np.loadtxt('./phonemes.txt',dtype=str)\n",
    "nphones = len(phonemes)\n",
    "nfeats = len(features)\n",
    "pbar = tqdm(subjs)\n",
    "for s in pbar:\n",
    "    pbar.set_description(f\"Creating stim/resp matrices for {s}\")\n",
    "    blockid = f'{s}_B1'\n",
    "    stims[s],resps[s] = dict(),dict()\n",
    "    # Make the resp\n",
    "    picks = mne.pick_types(raws[s].info, meg=False,eeg=True,stim=False)\n",
    "    resps[s] = raws[s].get_data(picks=picks)\n",
    "    nsamps = resps[s].shape[1]\n",
    "    # Make spkr stim\n",
    "    phn_stim_spkr = np.zeros((nphones, nsamps))\n",
    "    feat_stim_spkr = np.zeros((nfeats, nsamps))\n",
    "    el_sh_stim = np.zeros((2, nsamps))\n",
    "    el_times, sh_times = gk.get_el_sh_timing(git_path,s,'B1',mode='eeg')\n",
    "    for ev in spkr_events[s]:\n",
    "        onset = ev[0]\n",
    "        for el_time in el_times:\n",
    "            if onset >= el_time[0] and onset <= el_time[1]:\n",
    "                el_sh_stim[0,onset] = 1\n",
    "        for sh_time in sh_times:\n",
    "            if onset >= sh_time[0] and onset <= sh_time[1]:\n",
    "                el_sh_stim[1,onset] = 1\n",
    "        phn_label = ev[2]\n",
    "        phn_stim_spkr[phn_label,onset] = 1\n",
    "        phn_stripped = re.sub(r'\\d+', '', phonemes[phn_label].lower())\n",
    "        for fi, f in enumerate(features):\n",
    "            if phn_stripped in features_dict[f]:\n",
    "                feat_stim_spkr[fi,onset] = 1\n",
    "    # Make mic stim\n",
    "    phn_stim_mic = np.zeros((nphones, nsamps))\n",
    "    feat_stim_mic = np.zeros((nfeats, nsamps))\n",
    "    for ev in mic_events[s]:\n",
    "        onset = ev[0]\n",
    "        phn_label = ev[2]\n",
    "        phn_stim_mic[phn_label,onset] = 1\n",
    "        phn_stripped = re.sub(r'\\d+', '', phonemes[phn_label].lower())\n",
    "        for fi, f in enumerate(features):\n",
    "            if phn_stripped in features_dict[f]:\n",
    "                feat_stim_mic[fi,onset] = 1\n",
    "    # Concanate stimulus features \n",
    "    stims[s] = np.vstack((\n",
    "        (feat_stim_spkr + feat_stim_mic), feat_stim_spkr, feat_stim_mic,\n",
    "        np.atleast_2d(phn_stim_spkr.sum(0)), np.atleast_2d(phn_stim_mic.sum(0)),\n",
    "        el_sh_stim\n",
    "    )).T\n",
    "    try:\n",
    "        stims[s] = np.vstack((stims[s].T,emgs[s])).T\n",
    "    except:\n",
    "        warnings.warn(f'EMG for {s} not applied to model! Again, something is wrong...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664eb688",
   "metadata": {},
   "source": [
    "### Spilt stimulus and response matrices into training and validation set\n",
    "80% of the data are used for training, while the remaining 20% of data are held out for validating model performance. To mitigate any potential overfitting, data are split according to sentence boundary. Because there are 50 unique sentences in the task, that means 40 sentences are used in training while 10 are held out for validation.\n",
    "\n",
    "This cell can take a while to run depending on your hardware. The outputs are saved to an `.hdf5` file so that repeated iterations of model fitting do not require one to re-make the split data. This is convenient in terms of raw computational resources and also security in case of a kernel crash due to RAM overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83576bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 6655321\n",
    "tStims, vStims, tResps, vResps = dict(), dict(), dict(), dict()\n",
    "pbar = tqdm(subjs)\n",
    "for s in pbar:\n",
    "    blockid = f\"{s}_B1\"\n",
    "    # Update this path if you're saving/loading h5 files locally\n",
    "    h5_fpath = f'{h5_path}{s}_model_inputs.hdf5'\n",
    "    if os.path.isfile(h5_fpath):\n",
    "        pbar.set_description(f\"Stim/resp for {s} already split, skipping this subject...\")\n",
    "    else:\n",
    "        pbar.set_description(f\"Splitting stim/resp into training/validation sets for {s}\")\n",
    "        # Read event files\n",
    "        onsets, offsets, ids, = [], [], []\n",
    "        # Read event files (spkr)\n",
    "        spkr_sn_ev_fpath = f\"{git_path}eventfiles/{s}/{blockid}/{blockid}_spkr_sn_all.txt\"\n",
    "        with open(spkr_sn_ev_fpath,'r') as f:\n",
    "            c = csv.reader(f,delimiter='\\t')\n",
    "            for row in c:\n",
    "                onsets.append(int(float(row[0])*fs))\n",
    "                offsets.append(int(float(row[1])*fs))\n",
    "                ids.append(int(row[2]))\n",
    "        if s in ['OP0015','OP0016']:\n",
    "            # Load events from second block too\n",
    "            last_samp = mne.io.read_raw_brainvision(\n",
    "                f\"{eeg_data_path}{s}/{blockid}/{blockid}_downsampled.vhdr\",preload=False,verbose=False\n",
    "            ).last_samp\n",
    "            b2_blockid = f\"{s}_B2\"\n",
    "            b2_spkr_sn_ev_fpath = f\"{git_path}eventfiles/{s}/{b2_blockid}/{b2_blockid}_spkr_sn_all.txt\"\n",
    "            with open(b2_spkr_sn_ev_fpath,'r') as f:\n",
    "                c = csv.reader(f,delimiter='\\t')\n",
    "                for row in c:\n",
    "                    onsets.append(int(float(row[0])*fs)+last_samp)\n",
    "                    offsets.append(int(float(row[1])*fs)+last_samp)\n",
    "                    ids.append(int(row[2]))\n",
    "        # Read event files (mic)\n",
    "        mic_sn_ev_fpath = f\"{git_path}eventfiles/{s}/{blockid}/{blockid}_mic_sn_all.txt\"\n",
    "        with open(mic_sn_ev_fpath,'r') as f:\n",
    "            c = csv.reader(f,delimiter='\\t')\n",
    "            for row in c:\n",
    "                onsets.append(int(float(row[0])*fs))\n",
    "                offsets.append(int(float(row[1])*fs))\n",
    "                ids.append(int(row[2]))\n",
    "        if s in ['OP0015','OP0016']:\n",
    "            # Load events from second block too\n",
    "            b2_mic_sn_ev_fpath = f\"{git_path}eventfiles/{s}/{b2_blockid}/{b2_blockid}_mic_sn_all.txt\"\n",
    "            with open(b2_mic_sn_ev_fpath,'r') as f:\n",
    "                c = csv.reader(f,delimiter='\\t')\n",
    "                for row in c:\n",
    "                    onsets.append(int(float(row[0])*fs)+last_samp)\n",
    "                    offsets.append(int(float(row[1])*fs)+last_samp)\n",
    "                    ids.append(int(row[2]))\n",
    "        # Split events sentence-by-sentence\n",
    "        sn_events = dict()\n",
    "        for this_sentence in range(len(np.unique(ids))):\n",
    "            sn_ranges = []\n",
    "            for i, sn_id in enumerate(ids):\n",
    "                if sn_id == this_sentence:\n",
    "                    onset_samp = onsets[i]\n",
    "                    offset_samp = offsets[i]\n",
    "                    sn_ranges.append([onset_samp,offset_samp])\n",
    "            sn_events[this_sentence] = sn_ranges\n",
    "        # Split stim/resp sentence-by-sentence\n",
    "        resp_dict, stim_dict = dict(), dict()\n",
    "        for this_sentence in range(len(np.unique(ids))):\n",
    "            sn_resps, sn_stims = [], []\n",
    "            for i, ev in enumerate(sn_events[this_sentence]):\n",
    "                onset = ev[0]\n",
    "                offset = ev[1]\n",
    "                for samp_idx in np.arange(resps[s].shape[1]):\n",
    "                    if samp_idx >= onset and samp_idx <= offset:\n",
    "                        sn_resps.append(resps[s][:,samp_idx])\n",
    "                        sn_stims.append(stims[s][samp_idx])\n",
    "            resp_dict[this_sentence] = np.array(sn_resps)\n",
    "            stim_dict[this_sentence] = np.array(sn_stims)\n",
    "        # Split stim/resp into training/validation sets along sentence boundaries\n",
    "        nsentences = 50\n",
    "        tv_split = 40 # 40 sentences IDs to train, remaining 10 to validate\n",
    "        np.random.seed(random_seed)\n",
    "        train_sn_ids = np.random.permutation(nsentences)[:tv_split]\n",
    "        np.random.seed(random_seed)\n",
    "        val_sn_ids = np.random.permutation(nsentences)[tv_split:]\n",
    "        tStims_by_sn, vStims_by_sn, tResps_by_sn, vResps_by_sn = dict(), dict(), dict(), dict()\n",
    "        for this_sentence in train_sn_ids:\n",
    "            tResps_by_sn[this_sentence] = resp_dict[this_sentence]\n",
    "            tStims_by_sn[this_sentence] = stim_dict[this_sentence]\n",
    "        for this_sentence in val_sn_ids:\n",
    "            vResps_by_sn[this_sentence] = resp_dict[this_sentence]\n",
    "            vStims_by_sn[this_sentence] = stim_dict[this_sentence]\n",
    "        tStims[s] = np.vstack(list(tStims_by_sn.values()))\n",
    "        vStims[s] = np.vstack(list(vStims_by_sn.values()))\n",
    "        tResps[s] = np.vstack(list(tResps_by_sn.values()))\n",
    "        vResps[s] = np.vstack(list(vResps_by_sn.values()))\n",
    "        pbar.set_description(\n",
    "            f\"{s}: training on {tStims[s].shape[0]} samps, validating on {vStims[s].shape[0]} samps. Raw contained {stims[s].shape[0]} samps total\"\n",
    "        )\n",
    "        if tStims[s].shape[0] != tResps[s].shape[0]:\n",
    "            raise Exception(\"Stim and resp do not have the same shape! (training)\")\n",
    "        if vStims[s].shape[0] != vResps[s].shape[0]:\n",
    "            raise Exception(\"Stim and resp do not have the same shape! (validation)\")\n",
    "        # Save the split to hdf5 files\n",
    "        with h5py.File(h5_fpath,'a') as f:\n",
    "            f.create_dataset('tStim', data=tStims[s])\n",
    "            f.create_dataset('tResp', data=tResps[s])\n",
    "            f.create_dataset('vStim', data=vStims[s])\n",
    "            f.create_dataset('vResp', data=vResps[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866c5953",
   "metadata": {},
   "source": [
    "### Load training/validation stimulus/response matrices from `hdf5` file\n",
    "The training/validation response matrices are saved to hdf5 and loaded here. Training/validation stimulus matrices are also loaded here, but an additional step is required while loading. To conserve disk space, model inputs for all models described in the first markdown cell of this notebook are saved to a singlular `hdf5` file. Assuming we do not want to maximum number of stimulus features, we need to clip the loaded matrix down to the appropriate specification of stimulus features. We use a function from `./utils/strf.py` to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73537d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tStims, vStims, tResps, vResps = dict(), dict(), dict(), dict()\n",
    "for s in tqdm(subjs):\n",
    "    model_input_h5_fpath = f\"{h5_path}{s}_model_inputs.hdf5\"\n",
    "    tStims[s], tResps[s], vStims[s], vResps[s] = strf.load_model_inputs(model_input_h5_fpath, model_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905cfe9a",
   "metadata": {},
   "source": [
    "### Fit linear encoding model\n",
    "This cell takes a very long time to run depending on the number of stimulus features in the specified model and your local hardware. Go get a coffee (or several)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a38775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the STRF\n",
    "corrs, wts, best_alphas = dict(),dict(),dict()\n",
    "pbar = tqdm(subjs)\n",
    "for s in pbar:\n",
    "    pbar.set_description(f\"Fitting STRF for {s} {model_number}\")\n",
    "    blockid = s + '_B1'\n",
    "    subj_corrs, subj_wts, _, _, _, _, subj_best_alphas = strf.strf( # don't save stim/resp\n",
    "        tResps[s], tStims[s],\n",
    "        vResp = vResps[s], vStim = vStims[s],\n",
    "        nboots=nboots, delay_min=delay_min, delay_max=delay_max, alphas=alphas,\n",
    "        flip_resp = True\n",
    "    )\n",
    "    print(f\"{s}: Best alpha: {subj_best_alphas[0]} (should be in between {alphas[0]} and {alphas[-1]})\")\n",
    "    # Write model output to dicts\n",
    "    corrs[s] = subj_corrs[0] # R-value between predicted and actual EEG at each channel\n",
    "    wts[s] = subj_wts[0].reshape((ndelays,tStims[s].shape[1],tResps[s].shape[1])) # delays x feats x chans\n",
    "    best_alphas[s] = subj_best_alphas # Regularization parameter that yielded best model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020515fa",
   "metadata": {},
   "source": [
    "### Save results\n",
    "Model weights are saved to an `.hdf5` file stored locally, while correlations, best alpha, and p-values (calculated in `git/stats/bootstrap_lme.ipynb`) are saved to a `.csv` file stored on GitHub (`git/stats/lme_results.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights to hdf5 file\n",
    "force_overwrite = True\n",
    "pbar = tqdm(subjs)\n",
    "for s in pbar:\n",
    "    pbar.set_description(f\"Saving model results for {s} {model_number}\")\n",
    "    # Update this file location accordingly on your local machine!\n",
    "    model_output_h5_fpath = f\"{h5_path}{s}_weights.hdf5\"\n",
    "    if os.path.isfile(model_output_h5_fpath):\n",
    "        with h5py.File(model_output_h5_fpath,'a') as f:\n",
    "            if model_number not in f.keys():\n",
    "                f.create_dataset(model_number, data = wts[s])\n",
    "            elif force_overwrite:\n",
    "                f[model_number][:] = wts[s]\n",
    "            else:\n",
    "                print(\n",
    "                    f\"{s} {model_number} weights already saved to hdf5. To force an overwrite, set 'force_overwrite' to True.\"\n",
    "                )\n",
    "    else:\n",
    "        with h5py.File(model_output_h5_fpath,'w') as f:\n",
    "            f.create_dataset(model_number, data = wts[s])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b562857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save corrs, alphas to csv file\n",
    "force_overwrite = True\n",
    "results_csv_fpath = f\"{git_path}stats/lme_results.csv\"\n",
    "df = pd.read_csv(results_csv_fpath)\n",
    "pbar = tqdm(subjs)\n",
    "for s in pbar:\n",
    "    pbar.set_description(f\"Saving alphas/corrs for {s} {model_number} to csv\")\n",
    "    blockid = f\"{s}_B1\"\n",
    "    ch_names = mne.io.read_raw_brainvision(f\"{eeg_data_path}{s}/{blockid}/{blockid}_cca.vhdr\",\n",
    "                                           preload=False,verbose=False).info['ch_names']\n",
    "    for i,ch in enumerate(ch_names):\n",
    "        if len(df[(df['subject']==s) & (df['model']==model_number) & (df['channel']==ch)]) == 0:\n",
    "            # Data does not exist in dataframe so let's add it\n",
    "            # using nan as pval as a placeholder as we still need to bootstrap\n",
    "            new_row = pd.DataFrame({\n",
    "                'subject':[s], 'model':[model_number], 'channel':[ch],\n",
    "                'r_value':[corrs[s][i]], 'p_value':[\"nan\"], \"best_alpha\":[best_alphas[s][0]]\n",
    "            })\n",
    "            df = df.append(new_row,ignore_index=True)\n",
    "        elif force_overwrite:\n",
    "            tgt_row = df[(df['subject']==s) & (df['model']==model_number) & (df['channel']==ch)]\n",
    "            df.loc[tgt_row.index, 'r_value'] = corrs[s][i]\n",
    "            df.loc[tgt_row.index, 'best_alpha'] = best_alphas[s][0]\n",
    "            df.loc[tgt_row.index, 'p_value'] = \"nan\"\n",
    "        else:\n",
    "            print(\n",
    "                f\"data for {s} {model_number} {ch} already exists. To overwrite, set 'force_overwrite' to True\"\n",
    "            )\n",
    "df.to_csv(results_csv_fpath,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
