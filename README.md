# speaker_induced_suppression_EEG
Code for reproducing the results from Kurteff et al. "Speaker-induced suppression in EEG during a naturalistic reading and listening task"
Data available on [OSF](https://osf.io/fnrd9/).

Below is a breakdown of different files in this repo:

## `figures`
This folder contains code for reproducing figures from the manuscript.
* `fig1.ipynb` -- panel-by-panel reproduction of Figure 1 in Jupyter notebook.
* `fig2.ipynb` -- panel-by-panel reproduction of Figure 2 in Jupyter notebook.
* `fig3.ipynb` -- panel-by-panel reproduction of Figure 3 in Jupyter notebook.
* `fig4.ipynb` -- panel-by-panel reproduction of Figure 4 in Jupyter notebook.

## `stats`
This folder contains code for reproducing the statistical analyses present in the manuscript.
* `within_subject.ipynb` -- code for reproducing the within subject statistical tests in Jupyter notebook.
* `bootstrap_lem.ipynb` -- code for significance testing the linear encoding models via bootstrap.
* `./lme/` -- folder containing R scripts for reported linear mixed-effects models.
* `./lme/create_csvs.ipynb` -- code for creating csv files associated with reported linear mixed-effects models.
* `./lme/csvs/` -- folder containing the csvs used in fitting linear mixed-effects models. These are generated by `./lme/create_csvs.ipynb`.

## `preprocessing`
This folder contains code for preprocessing raw EEG data. CCA correction was performed in EEGLab using settings outlined in the manuscript, but all other preprocessing was accomplished through these scripts. If you do not wish to run these scripts, you can download the preprocessed datasets (or the raw ones) from the OSF repository.
* `convert_fif_to_vhdr.py` -- Python wrapper script for philistine that converts an MNE-created .fif file to .vhdr for use in EEGLab.
*  `ica.py` -- Python script meant to be ran in an interactive shell for doing ICA. Uses MNE-python's ICA functions as a backend. You will need to manually edit some paths if you are running this script.
* `train_linear_model.ipynb` -- code for training the linear encoding model in Jupyter notebook.

## `task`
This folder contains mostly Swift code for running the perception-production task on an iPad during data collection.
* `sampling_MOCHA.ipynb` -- Python notebook that describes how the subset of MOCHA used in the task was sampled.

## `eventfiles`
This folder contains the event files used to epoch the data. The following aliases are used in the filenames to denote task condition:
* `spkr`: perception
* `mic`: production
* `el` : predictable
* `sh` : unpredictable

## `textgrids`
This folder contains the phone, word, and sentence-level Praat TextGrids for each subject.

## Third-party software used
* [philistine](https://pypi.org/project/philistine/)
* [EEGLab](https://sccn.ucsd.edu/eeglab/index.php)
* [AAR plugin for EEGLab](https://germangh.github.io/eeglab_plugin_aar/)
* [MNE-python](https://mne.tools/dev/index.html)
