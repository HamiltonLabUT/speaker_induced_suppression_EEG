{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca2a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import csv\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../../preprocessing/utils/')\n",
    "import strf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba855375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these path for running the notebook locally\n",
    "eeg_data_path = '/path/to/dataset/' # downloadable from OSF: https://doi.org/10.17605/OSF.IO/FNRD9\n",
    "git_path  = '/path/to/git/speaker_induced_suppression_EEG/'\n",
    "# Where the output of train_linear_model.ipynb is saved. Run that first if you haven't already.\n",
    "h5_path = '/path/to/h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4b85ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "picks = ['F1','Fz','F2','FC1','FCz','FC2','C1','Cz','C2']\n",
    "tmin,tmax = -1.5, 3.5\n",
    "baseline=(None,0)\n",
    "reject_thresh = 10 # SD\n",
    "exclude = 'OP0020'\n",
    "subjs = np.sort([s[-6:] for s in glob(f'{git_path}eventfiles/*') if 'OP0' in s and exclude not in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41324e89",
   "metadata": {},
   "source": [
    "## Load ERP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5478a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_bads(subj,block,raw,git_path,eeg_data_path):\n",
    "    '''\n",
    "    CCA .vhdr files do not include bad channels, so this func interpolates them.\n",
    "    The end result is each subject has a 64 channel raw arrray, which makes plotting easier\n",
    "    Filter the data before running this!\n",
    "    '''\n",
    "    blockid = subj + '_' + block\n",
    "    nsamps = len(raw)\n",
    "    info = mne.io.read_raw_brainvision(f'{eeg_data_path}OP0001/OP0001_B1/OP0001_B1_cca.vhdr',\n",
    "                                           preload=False,verbose=False).info\n",
    "    ch_names = info['ch_names']\n",
    "    bads = [c for c in ch_names if c not in raw.info['ch_names']]\n",
    "    if len(bads) > 0:\n",
    "        new_data = []\n",
    "        for ch in ch_names:\n",
    "            if ch in bads:\n",
    "                print('Interpolating', ch)\n",
    "                new_data.append(np.zeros((1,nsamps)))\n",
    "            else:\n",
    "                new_data.append(raw.get_data(picks=ch)[0])\n",
    "        raw = mne.io.RawArray(np.vstack(new_data),info)\n",
    "        raw.info['bads'] = bads\n",
    "        raw.interpolate_bads()\n",
    "    else:\n",
    "        print(subj, 'has no bads.')\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raws, spkr_epochs, mic_epochs, spkr_resp, mic_resp = dict(), dict(), dict(), dict(), dict()\n",
    "conditions = ['el','sh','all']\n",
    "channels = ['spkr', 'mic']\n",
    "conditions\n",
    "subj_bar = tqdm(subjs)\n",
    "for s in subj_bar:\n",
    "    spkr_epochs[s], mic_epochs[s], spkr_resp[s], mic_resp[s] = dict(), dict(), dict(), dict()\n",
    "    if s != 'OP0002':\n",
    "        block = 'B1'\n",
    "    else:\n",
    "        block = 'B2'\n",
    "    blockid = s + '_' + block\n",
    "    raw_path = f'{eeg_data_path}{s}/{blockid}/{blockid}_cca.vhdr'\n",
    "    raw = mne.io.read_raw_brainvision(raw_path,preload=True,verbose=False)\n",
    "    raw.filter(l_freq=1,h_freq=30,verbose=False)\n",
    "    raws[s] = interpolate_bads(s,block,raw,git_path,eeg_data_path)\n",
    "    for ch in channels:\n",
    "        spkr_epochs[s][ch], mic_epochs[s][ch], spkr_resp[s][ch], mic_resp[s][ch] = dict(), dict(), dict(), dict()\n",
    "        for co in conditions:\n",
    "            subj_bar.set_description(f'Epoching {s} {ch} {co}')\n",
    "            if s in ['OP0015','OP0016']: # mtpl blocks\n",
    "                events = []\n",
    "                for b in ['B1','B2']: \n",
    "                    event_file = f'{git_path}eventfiles/{s}/{s}_{b}/{s}_{b}_{ch}_sn_{co}.txt'\n",
    "                    if b == 'B2':\n",
    "                        b2_fpath = raw_path = f'{eeg_data_path}{s}/{s}_B1/{s}_B1_downsampled.vhdr'\n",
    "                        b2_raw = mne.io.read_raw_brainvision(b2_fpath,preload=True,verbose=False)\n",
    "                        block_shift = b2_raw.last_samp\n",
    "                    else:\n",
    "                        block_shift = 0\n",
    "                    with open(event_file, 'r') as my_csv:\n",
    "                        csvReader = csv.reader(my_csv, delimiter='\\t')\n",
    "                        for row in csvReader:\n",
    "                            onset = int((float(row[0])*128)+block_shift)\n",
    "                            offset = int((float(row[1])*128)+block_shift)\n",
    "                            sn_id = int(row[2])\n",
    "                            events.append([onset,offset,sn_id])\n",
    "                events = np.array(events,dtype=int)\n",
    "            else:\n",
    "                event_file = f'{git_path}eventfiles/{s}/{blockid}/{blockid}_{ch}_sn_{co}.txt'\n",
    "                events = []\n",
    "                with open(event_file, 'r') as my_csv:\n",
    "                    csvReader = csv.reader(my_csv, delimiter='\\t')\n",
    "                    for row in csvReader:\n",
    "                        onset = int((float(row[0])*128))\n",
    "                        offset = int((float(row[1])*128))\n",
    "                        sn_id = int(row[2])\n",
    "                        events.append([onset,offset,sn_id])\n",
    "                events = np.array(events,dtype=int)\n",
    "            reject = mne.Epochs(raws[s], events, tmin=tmin, tmax=tmax,reject=None,\n",
    "                           baseline=baseline,verbose=False)\n",
    "            reject = reject.get_data(picks=picks)\n",
    "            reject = dict(eeg=np.std(reject)*(reject_thresh*2))\n",
    "            epochs = mne.Epochs(raws[s],events,tmin=tmin,tmax=tmax,reject=reject,\n",
    "                                   baseline=baseline,verbose=False)\n",
    "            if ch == 'spkr':\n",
    "                spkr_epochs[s][ch][co] = epochs\n",
    "                spkr_resp[s][ch][co] = epochs.get_data(picks=picks)\n",
    "            if ch == 'mic':\n",
    "                mic_epochs[s][ch][co] = epochs\n",
    "                mic_resp[s][ch][co] = epochs.get_data(picks=picks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d8575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract windows of interest\n",
    "N100_window = [0.08,0.15]\n",
    "P200_window = [0.15,0.25]\n",
    "t = np.linspace(tmin,tmax,num=spkr_resp['OP0001']['spkr']['all'].shape[2])\n",
    "N100_inds = np.where((t>N100_window[0]) & (t<N100_window[1]))[0]\n",
    "P200_inds = np.where((t>P200_window[0]) & (t<P200_window[1]))[0]\n",
    "N100_spkr_peak_amplitude,N100_mic_peak_amplitude = dict(),dict()\n",
    "P200_spkr_peak_amplitude,P200_mic_peak_amplitude = dict(),dict()\n",
    "N100_spkr_peak_latency,N100_mic_peak_latency = dict(),dict()\n",
    "P200_spkr_peak_latency,P200_mic_peak_latency = dict(),dict()\n",
    "peak_to_peak_spkr,peak_to_peak_mic = dict(),dict()\n",
    "for subj in subjs:\n",
    "    # SPKR\n",
    "    # Get the minimum value at each epoch\n",
    "    N100_spkr_peak_amplitude[subj] = spkr_resp[subj]['spkr']['all'].mean(1)[:,N100_inds].min(1) # amplitude\n",
    "    latency_idx = N100_inds[spkr_resp[subj]['spkr']['all'].mean(1)[:,N100_inds].argmin(1)] # latency\n",
    "    N100_spkr_peak_latency[subj] = t[latency_idx]\n",
    "    P200_spkr_peak_amplitude[subj] = spkr_resp[subj]['spkr']['all'].mean(1)[:,P200_inds].max(1) # amplitude\n",
    "    latency_idx = P200_inds[spkr_resp[subj]['spkr']['all'].mean(1)[:,P200_inds].argmax(1)] # latency\n",
    "    P200_spkr_peak_latency[subj] = t[latency_idx]\n",
    "    peak_to_peak_spkr[subj] = np.abs(\n",
    "        P200_spkr_peak_amplitude[subj] - N100_spkr_peak_amplitude[subj])\n",
    "    # MIC\n",
    "    N100_mic_peak_amplitude[subj] = mic_resp[subj]['mic']['all'].mean(1)[:,N100_inds].min(1) # amplitude\n",
    "    latency_idx = N100_inds[mic_resp[subj]['mic']['all'].mean(1)[:,N100_inds].argmin(1)] # latency\n",
    "    N100_mic_peak_latency[subj] = t[latency_idx]\n",
    "    P200_mic_peak_amplitude[subj] = mic_resp[subj]['mic']['all'].mean(1)[:,P200_inds].max(1) # amplitude\n",
    "    latency_idx = P200_inds[mic_resp[subj]['mic']['all'].mean(1)[:,P200_inds].argmax(1)] # latency\n",
    "    P200_mic_peak_latency[subj] = t[latency_idx]\n",
    "    peak_to_peak_mic[subj] = np.abs(\n",
    "        P200_mic_peak_amplitude[subj] - N100_mic_peak_amplitude[subj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512bbcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "N100_el_peak_amplitude,N100_sh_peak_amplitude = dict(),dict()\n",
    "P200_el_peak_amplitude,P200_sh_peak_amplitude = dict(),dict()\n",
    "N100_el_peak_latency,N100_sh_peak_latency = dict(),dict()\n",
    "P200_el_peak_latency,P200_sh_peak_latency = dict(),dict()\n",
    "peak_to_peak_el,peak_to_peak_sh = dict(),dict()\n",
    "for subj in subjs:\n",
    "    if subj != 'OP0015': # no sh\n",
    "        # el\n",
    "        N100_el_peak_amplitude[subj] = spkr_resp[subj]['spkr']['el'].mean(1)[:,N100_inds].min(1) # amplitude\n",
    "        latency_idx = N100_inds[spkr_resp[subj]['spkr']['el'].mean(1)[:,N100_inds].argmin(1)] # latency\n",
    "        N100_el_peak_latency[subj] = t[latency_idx]\n",
    "        P200_el_peak_amplitude[subj] = spkr_resp[subj]['spkr']['el'].mean(1)[:,P200_inds].max(1) # amplitude\n",
    "        latency_idx = P200_inds[spkr_resp[subj]['spkr']['el'].mean(1)[:,P200_inds].argmax(1)] # latency\n",
    "        P200_el_peak_latency[subj] = t[latency_idx]\n",
    "        peak_to_peak_el[subj] = np.abs(\n",
    "            P200_el_peak_amplitude[subj] - N100_el_peak_amplitude[subj])\n",
    "        # sh\n",
    "        N100_sh_peak_amplitude[subj] = spkr_resp[subj]['spkr']['sh'].mean(1)[:,N100_inds].min(1) # amplitude\n",
    "        latency_idx = N100_inds[spkr_resp[subj]['spkr']['sh'].mean(1)[:,N100_inds].argmin(1)] # latency\n",
    "        N100_sh_peak_latency[subj] = t[latency_idx]\n",
    "        P200_sh_peak_amplitude[subj] = spkr_resp[subj]['spkr']['sh'].mean(1)[:,P200_inds].max(1) # amplitude\n",
    "        latency_idx = P200_inds[spkr_resp[subj]['spkr']['sh'].mean(1)[:,P200_inds].argmax(1)] # latency\n",
    "        P200_sh_peak_latency[subj] = t[latency_idx]\n",
    "        peak_to_peak_sh[subj] = np.abs(P200_sh_peak_amplitude[subj] - N100_sh_peak_amplitude[subj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe02e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to CSV\n",
    "csvheader = [['Subject','Cond','N100_amp','P200_amp','N100_latency','P200_latency','peak_to_peak']]\n",
    "csv_fname = f'{git_path}stats/lme/csvs/perception_production.csv'\n",
    "conditions = ['Perc','Prod']\n",
    "for i,subj in enumerate(subjs):\n",
    "    for condition in conditions:\n",
    "        if condition == 'Perc':\n",
    "            for i,trial in enumerate(N100_spkr_peak_amplitude[subj]):\n",
    "                csvheader.append([subj,condition,\n",
    "                                  trial,P200_spkr_peak_amplitude[subj][i],\n",
    "                                  N100_spkr_peak_latency[subj][i],P200_spkr_peak_latency[subj][i],\n",
    "                                  peak_to_peak_spkr[subj][i]])\n",
    "        elif condition == 'Prod':\n",
    "            for i,trial in enumerate(N100_mic_peak_amplitude[subj]):\n",
    "                csvheader.append([subj,condition,\n",
    "                                  trial,P200_mic_peak_amplitude[subj][i],\n",
    "                                  N100_mic_peak_latency[subj][i],P200_mic_peak_latency[subj][i],\n",
    "                                  peak_to_peak_mic[subj][i]])\n",
    "with open(csv_fname,'w+') as my_csv:\n",
    "    csvWriter = csv.writer(my_csv,delimiter=',')\n",
    "    csvWriter.writerows(csvheader)\n",
    "\n",
    "csvheader = [['Subject','Cond','N100_amp','P200_amp','N100_latency','P200_latency','peak_to_peak']]\n",
    "csv_fname = f'{git_path}stats/lme/csvs/predictable_unpredictable.csv'\n",
    "conditions = ['Echo','Shuff']\n",
    "for i,subj in enumerate(subjs):\n",
    "    if subj != 'OP0015': # no sh\n",
    "        for condition in conditions:\n",
    "            if condition == 'Echo':\n",
    "                for i,trial in enumerate(N100_el_peak_amplitude[subj]):\n",
    "                    csvheader.append([subj,condition,\n",
    "                                      trial,P200_el_peak_amplitude[subj][i],\n",
    "                                      N100_el_peak_latency[subj][i],P200_el_peak_latency[subj][i],\n",
    "                                      peak_to_peak_el[subj][i]])\n",
    "            elif condition == 'Shuff':\n",
    "                for i,trial in enumerate(N100_sh_peak_amplitude[subj]):\n",
    "                    csvheader.append([subj,condition,\n",
    "                                      trial,P200_sh_peak_amplitude[subj][i],\n",
    "                                      N100_sh_peak_latency[subj][i],P200_sh_peak_latency[subj][i],\n",
    "                                      peak_to_peak_sh[subj][i]])\n",
    "with open(csv_fname,'w+') as my_csv:\n",
    "    csvWriter = csv.writer(my_csv,delimiter=',')\n",
    "    csvWriter.writerows(csvheader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf1896",
   "metadata": {},
   "source": [
    "## Load LEM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6e4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['model1','model1e','model2','model2e','model3','model3e','model4','model4e']\n",
    "exclude = ['OP0001','OP0002','OP0020']\n",
    "subjs = [s for s in subjs if s not in exclude]\n",
    "tmin,tmax = -0.3,0.5\n",
    "delays = np.arange(np.floor(tmin*128),np.ceil(tmax*128),dtype=int)\n",
    "features = {model_number:strf.get_feats(model_number,extend_labels=True) for model_number in models}\n",
    "n_feats = {model_number:len(features[model_number]) for model_number in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf67e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from hdf5, pandas\n",
    "wts, corrs, pvals, sig_wts, sig_corrs, alphas = dict(), dict(), dict(), dict(), dict(), dict()\n",
    "results_csv_fpath = f\"{git_path}stats/lem_results.csv\"\n",
    "df = pd.read_csv(results_csv_fpath)\n",
    "for m in models:\n",
    "    wts[m], corrs[m], pvals[m], sig_wts[m], sig_corrs[m], alphas[m] = dict(), dict(), dict(), dict(), dict(), dict()\n",
    "    b = tqdm(subjs)\n",
    "    for s in b:\n",
    "        blockid = f\"{s}_B1\"\n",
    "        b.set_description(f'Loading STRF for {s} {m}')\n",
    "        with h5py.File(f\"{h5_path}{s}_weights.hdf5\",'r') as f:\n",
    "            wts[m][s] = np.array(f.get(m))\n",
    "        ch_names = mne.io.read_raw_brainvision(f\"{eeg_data_path}{s}/{blockid}/{blockid}_cca.vhdr\",\n",
    "                                               preload=False,verbose=False).info['ch_names']\n",
    "        subj_corrs, subj_best_alphas, subj_pvals = np.zeros(len(ch_names)), np.zeros(len(ch_names)), np.zeros(len(ch_names))\n",
    "        for i, ch in enumerate(ch_names):\n",
    "            tgt_row = df[(df['subject']==s) & (df['model']==m) & (df['channel']==ch)]\n",
    "            subj_corrs[i] = df.loc[tgt_row.index, 'r_value']\n",
    "            subj_best_alphas[i] = df.loc[tgt_row.index, 'best_alpha']\n",
    "            subj_pvals[i] = df.loc[tgt_row.index, 'p_value']\n",
    "        corrs[m][s] = np.array(subj_corrs)\n",
    "        pvals[m][s] = np.array(subj_pvals)\n",
    "        alphas[m][s] = np.array(subj_best_alphas)\n",
    "    # Extract significant weights, corrs\n",
    "    for s in subjs:\n",
    "        nchans = wts[m][s].shape[2]\n",
    "        sig_wts[m][s] = np.zeros((len(delays),n_feats[m],nchans))\n",
    "        sig_corrs[m][s] = np.zeros((nchans))\n",
    "        for i in np.arange(nchans):\n",
    "            if pvals[m][s][i] < 0.01:\n",
    "                sig_wts[m][s][i] = wts[m][s][i]\n",
    "                sig_corrs[m][s][i] = corrs[m][s][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "xm, ym = 'model1', 'model2'\n",
    "header = [['r','model','subject','channel']]\n",
    "for s in subjs:\n",
    "    ch_names = mne.io.read_raw_brainvision(\n",
    "        f'{eeg_data_path}{s}/{s}_B1/{s}_B1_cca.vhdr',\n",
    "        preload=False,verbose=False).info['ch_names']\n",
    "    for i,c in enumerate(ch_names):\n",
    "        header.append([corrs[xm][s][i],xm,s,c])\n",
    "        header.append([corrs[ym][s][i],ym,s,c])\n",
    "csv_fname = f'{git_path}stats/lme/csvs/{xm}_{ym}.csv'\n",
    "with open(csv_fname,'w+') as f:\n",
    "    csvWriter = csv.writer(f)\n",
    "    csvWriter.writerows(header)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
